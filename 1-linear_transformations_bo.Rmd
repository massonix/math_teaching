---
title: "Hands on session 1: Linear Transformations"
author: "Ferran Mui√±os and Ramon Massoni-Badosa"
date: "7/2/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, out.width = "100%", fig.align='center', 
                      message=FALSE, warning = FALSE)
options(width = 1200)
```

# Introduction and objectives

This session will focus on linear maps. Specifically, we aim to accomplish the following objectives:

1. Leverage the geometry of linear maps to understand how the concepts of rank, dimension of the null space, determinant and inverse matrices are connected
2. Create, subset, join and visualize matrices in R. Perform basic linear algebra operations (addition, multiplication, etc.)


# Pre-requisites

## Material

Watch the following videos:

- [Linear transformations and matrices, by 3blue1brown](https://www.youtube.com/watch?v=kYB8IZa5AuE&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&index=3).
- [Matrix multiplication as composition](https://www.youtube.com/watch?v=XkY2DOUCWMU)
- [More on linear independence](https://en.khanacademy.org/math/linear-algebra/vectors-and-spaces/linear-independence/v/more-on-linear-independence)
- [Relation between linear independence and null space](https://en.khanacademy.org/math/linear-algebra/vectors-and-spaces/null-column-space/v/null-space-3-relation-to-linear-independence)


Do the following exercises:

- [Guess the matrix that describes the transformation](https://en.khanacademy.org/math/linear-algebra/matrix-transformations/linear-transformations/a/practice-associating-matrices-with-transformations).


Check that you can access the following shiny app:

[https://massonix.shinyapps.io/linear_transformations/](https://massonix.shinyapps.io/linear_transformations/)


## Install software

- [R 4.0](https://cran.r-project.org/doc/manuals/r-devel/R-admin.html)
- [Rstudio](https://rstudio.com/products/rstudio/download/)
- [Bioconductor](https://www.bioconductor.org/install/)
- [pHeatmap](https://cran.r-project.org/web/packages/pheatmap/pheatmap.pdf)
- [Seurat](https://cran.r-project.org/web/packages/Seurat/index.html)
- [SeuratData](https://github.com/satijalab/seurat-data)


# Revision of key concepts

## Linear transformations

Simply put, we can think of a transformation as a function: something that takes an input vector and "spits" an output vector. In addition, they are considered linear if they fulfill these requirements:

1. The transformation of a sum of vectors equals the sum of the transformations:

$$
L(x + y) = L(x) + L(y)
$$

2. The transformation of a vector times a scalar equals the scalar times the transformed vector:

$$
L(\lambda x) = \lambda L(x)
$$

```{r echo=FALSE}
x <- matrix(c("x1", "x2"), nrow = 2)
i_hat <- matrix(c(1, 0), nrow = 2)
j_hat <- matrix(c(0, 1), nrow = 2)
write_matex2 <- function(x) {
  begin <- "\\begin{bmatrix}"
  end <- "\\end{bmatrix}"
  X <-
    apply(x, 1, function(x) {
      paste(
        paste(x, collapse = "&"),
        "\\\\"
      )
    })
  paste(c(begin, X, end), collapse = "")
}
```

To illustrate this, consider the vector following vector $x = `r write_matex2(x)`$, which can be expressed as a linear combination of the canonical basis vectors:

$$
x = x_1 `r write_matex2(i_hat)` + x_2 `r write_matex2(j_hat)`
$$

Let us take the linear transformation L of the vector x and apply the two properties of linear transformations::

$$
L(x) = L \left( x_1 `r write_matex2(i_hat)` + x_2 `r write_matex2(j_hat)`\right) = L \left(x_1 `r write_matex2(i_hat)`\right) + L \left(x_2 `r write_matex2(j_hat)`\right) = x_1 L\left(`r write_matex2(i_hat)`\right) + x_2 L\left(`r write_matex2(j_hat)`\right)
$$

From the last result, we can deduce two key facts:

1. Linear transformations are completely determined by where the basis vectors "land".
2. Linear transformation can be expresed as a matrix-vector product (Ax = b).

The (2) fact can be expressed as follows:

$$
Ax = b
$$
$$
x_1 a_1 + x_2 a_2 = b
$$
Where a<sub>1</sub> and a<sub>2</sub> are the column vectors of A.


## Important definitions

* **Column space**: the colum space of a matrix A (C(A)) is the vector space spanned by the column vectors of A.
* **Rank**: number of linearly independent columns. In other words, the dimension of C(A).
* **Determinant**: factor by which a unit of length, area or volume is multiplied in a linear transformation.
* **Inverse matrix**: matrix that reverses a linear map. That is, if Ax = b, A^-1^b = x.
* **Null space**: all the vectors x that satisfy Ax = 0. That is, the set of vectors that are mapped to 0 in a linear transformation.

Again, the main objective is **to develop the geometric intuition of linear transformations to understand how these concepts are interconnected**:
![](img/linear_transformations_shed_light.png)

There are two of these relations that useful to understand algebraically:

1. Relation between rank and dim(N(A)):

$$
a_1, ... , a_n \hspace{0.2cm} are \hspace{0.2cm} linearly \hspace{0.3cm} independent \Longleftrightarrow N(A) = {0} 
$$

In words: a matrix A is full rank (= all column vectors are linearly independent) if and only if the null space only contains the 0 vector. You can find the proof of this theorem in the [Khan Academy](https://en.khanacademy.org/) videos in the prerequisites sections.

2. Relation between dim(N(A)) and invertibility: 

$$
\forall A_{nxn} \hspace{0.2cm} {\displaystyle \exists !} \hspace{0.2cm}A^{-1} \Longleftrightarrow N(A) = {0} 
$$

In words: for every square matrix A, there exists one and only one matrix A^-1^ if and only if the null space only contains the 0 vector. You can find the proof of this theorem in the [third lecture](https://www.youtube.com/watch?v=FX4C-JpTFgY) of the MIT course in linear algebra, by Gilbert Strang (timestamps: 27:56-31:50).


