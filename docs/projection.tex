%-----------------------------------------------------------------------
% Beginning of proc-l-template.tex
%-----------------------------------------------------------------------
%
%     This is a topmatter template file for PROC for use with AMS-LaTeX.
%
%     Templates for various common text, math and figure elements are
%     given following the \end{document} line.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%     Remove any commented or uncommented macros you do not use.

\documentclass{proc-l}

%     If you need symbols beyond the basic set, uncomment this command.

\usepackage{amssymb}
\usepackage{amsmath}

%     If your article includes graphics, uncomment this command.
%\usepackage{graphicx}

%     If the article includes commutative diagrams, ...
%\usepackage[cmtip,all]{xy}


%     Update the information and uncomment if AMS is not the copyright
%     holder.

\copyrightinfo{2020}{}

\newtheorem{theorem}{Theorem}[subsection]
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{\bf Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{xca}[theorem]{Exercise}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\numberwithin{equation}{section}

%aliases

\newcommand{\comment}[1]{}
\newcommand{\R}{\mathbb{R}}
\newcommand{\rank}[1]{\textrm{rank}({#1})}
\newcommand{\im}[1]{\textrm{Im}({#1})}
\newcommand{\x}{\times}
\renewcommand{\ker}[1]{\textrm{ker}({#1})}



\begin{document}

% \title[short text for running head]{full title}
\title{Systems of linear equations can be regarded as projections}

%    Only \author and \address are required; other information is
%    optional.  Remove any unused author tags.

%    author one information
% \author[short version for running head]{name for top of paper}
\author{Ramon Massoni}
\address{}
\curraddr{}
\email{}
\thanks{}

%    author two information
\author{Ferran Mui\~nos}
\address{}
\curraddr{}
\email{}
\thanks{}

%    \subjclass is required.
%    \subjclass[2010]{Primary }

%     \date{\today}

%     \dedicatory{}

%    "Communicated by" -- provide editor's name; required.
%     \commby{}

%    Abstract is required.
\begin{abstract}
Solving a system of linear equations is equivalent to conducting an orthogonal projection in a sense that will be made precise below.
\end{abstract}

\maketitle

\section{Problem}

\subsection{}
Given a matrix $A\in \R^{m \x n}$ and a vector $b\in \R^m$, find a vector $v\in\R^n$ such that $Av=b$.

\subsection{} 
This problem has a solution if and only if $\rank{A} = \rank{A\,|\,b}$, or equivalently, if $b \in \textrm{span}\{A_1, \ldots, A_n\}$.

\subsection{} 
If $A$ is a full-rank, square matrix, then $A^{-1}$ exists and the solution can be given as follows $\hat{v} = A^{-1}b$.

\subsection{}
The entries of a solution vector $\hat{v} = (v_1, \ldots, v_n)$ can be thought of as a concrete way of carrying out a linear combination of the column vectors of $A$.

\subsection{}
What if $\rank{A} < \rank{A\,|\,b}$? In this case $b\notin\textrm{span}\{A_1,\ldots,A_n\}$, i.e., $b$ cannot be given as a linear combination of the column vectors of $A$.

\subsection{}\label{transform}
One possible way of relaxing the problem in the later situation would go as follows: we no longer ask for a solution vector $\hat v$ such that $A\hat v = b$, but such that $A\hat v - \pi(b) = 0$, with $\pi(b)$ being some convenient transform of $b$ such that: 
\begin{itemize}
\item[i] $\pi(b)$ is not too far from $b$;
\item[ii] $\rank{A} = \rank{A\,|\,\pi(b)}$.
\end{itemize}

\subsection{}
Whenever they exist, the solutions of the strict version of the problem should be solutions for the more relaxed version, although we cannot expect the converse to be true in general.

\section{Orthogonal Projection}

\subsection{}
A linear map $\pi_v: V\to V$ is deemed an orthogonal projection onto a vector $v$ if it maps $v$ onto itself and any vector orthogonal to $v$ to zero.

\subsection{}
What is the matrix of $\pi_v$? It depends on the basis of $V$ we employ to represent vector coordinates. Fixing an orthonormal basis of $V$, the matrix of $\pi_v$ is simply the rank one square matrix given by $u u^t$, where $u=v/\|v\|$.

\subsection{}
Let $S$ be a vector subspace of $V$. We can define the orthogonal projection onto $S$ as the linear map $\pi_S: V\to V$ that maps any vector of $S$ to itself and any vector orthogonal to $S$ to zero. 

\subsection{}
Suppose that $S=\textrm{span}\{u_1,\ldots, u_n\}$ and let's further assume that the vectors $u_1,\ldots, u_n$ are an orthogonal set. Then the orthogonal projection onto $S$ is given by the following linear map:
\begin{equation}\label{defproj}
\pi_{S} = \sum_{i=1}^n \pi_{u_i} = \sum_{i=1}^n \frac{1}{\| u_i \|^2} u_i u_i^t.
\end{equation}

\subsection{}
First, we must check that any vector $v$ of $S$ is mapped to itself by $\pi_S$ thus defined. Since $v$ belongs to $S$ we can write $v = \lambda_1 u_1 + \ldots + \lambda_n u_n$. Then

\begin{align*}
\pi_S(v) &=  \sum_{i=1}^n \frac{1}{\| u_i \|^2} u_i u_i^t v\\ 
&= \sum_{i=1}^n \frac{\lambda_i}{\| u_i \|^2} u_i u_i^t u_i = \sum_{i=1}^n \frac{\lambda_i \| u_i \|^2}{\| u_i \|^2} u_i\\
&= \sum_{i=1}^n \lambda_i u_i = v.
\end{align*}

\subsection{}
Second, any vector $v$ orthogonal to $S$ is mapped to zero by $\pi_S$. To justify that note that if $v$ is orthogonal to $S$, it is orthogonal to all its generators.

\subsection{}
The former projection $\pi_S$ as written in equation \ref{defproj} can be written in matrix form as $A(A^tA)^{-1}A^t$, where $A$ denotes the matrix with $u_1,\ldots,u_n$ as column vectors.

\subsection{}
Note that if the vectors $u_1, \ldots, u_n$ were an orthonormal set, then the matrix expression for $\pi_S$ would be just $AA^t$, since the middle factor $A^tA$ would be the identity matrix.

\subsection{}\label{null-remark}
Remark that if a vector $u$ is in the null space of $\pi_S$, then it must be orthogonal to $S$. Can you tell why?

\subsection{}
Now assume that the column vectors of $A$ are linearly independent, but not necessarily orthonormal. Then we know there is a full-rank, square matrix $E$ such that the matrix $Q=AE$ has orthonormal vectors as columns. Then the projection $\pi_S$ has the following matrix expression:

\begin{align*}
QQ^t & = Q(Q^tQ)^{-1}Q^t = (AE)((AE)^t(AE))^{-1}(AE)^t\\ 
&= AE(E^tA^tAE)^{-1}E^tA^t = AEE^{-1}(A^t A)^{-1}(E^t)^{-1}E^tA^t\\ 
&= A(A^tA)^{-1}A^t
\end{align*}

\subsection{}
In other words, the matrix of the orthogonal projection onto a vector subspace $S$ does not depend on the choice of a linearly independent set spanning $S$. Take any matrix $A$ with a linearly independent generating set of $S$ as columns, then the matrix of the orthogonal projection onto $S$ is just $A(A^tA)^{-1}A^t$.

\section{Back to the problem}

\subsection{}
Can the reader suggest one possible transform that serves the purpose of solving the linear system problem with the requirements referred to in section \ref{transform}? The orthogonal projection onto the column space of $A$ will do. Let's denote this linear map as $\pi_A$.

\subsection{}
The orthogonal projection of $b$ onto the column space of $A$ can be given as $\pi_A(b) = A(A^tA)^{-1}A^tb$. In view of this expression, we can already conclude that the vector $\hat v = (A^tA)^{-1}A^tb$ is a solution to the problem, i.e., $A\hat v = \pi_A(b)$. If $A$ has linearly independent columns, then $\hat v$ is the unique solution.

\subsection{}
Last but not least, it remains to be discussed what the ``not too far'' part from the requirements in section \ref{transform} is achieved with the $\pi_A$ projection.

\subsection{}
Given a vector subspace $S\subset V$ and any vector $v\in V$, then the orthogonal projection $\hat v = \pi_S(v)$ is the closest possible vector in $S$ to $v$, in other words, $\|w-v\| \geq \|\pi_S(v) - v\|$ for any $w\in S$. Let's prove it:
\\

\noindent
Suppose the non-trivial case where $v\notin S$. Then $v = \pi_S(v) + u$ and we can conclude that $u\in\ker{\pi_S}$, which in turn implies that $u$ is orthogonal to $S$. On the other hand, for any $w\in S$ we have $w = \pi_S(v) + r$, for some other $r\in S$. Then $w - v = \pi_S(v) + r - \pi_S(v) + u = r + u$ with $u$ orthogonal to $r$, whereas $\pi_S(v) - v = u$. Consequently,
\[
\|w - v\|^2 = (r + u)^t(r + u) = \|r\|^2 + \|u\|^2 \geq \|\pi_S(v) - v\|^2 = \|u\|^2.
\]

\section{A more general perspective on projections}
\subsection{}\label{projection}
Although not explicitly stated, we have seen that $\pi_S:V\to V$ is a linear map with a remarkable property: whenever composed with itself, it gives the same linear map, i.e., $\pi_S \circ \pi_S = \pi_S$. 

\subsection{}
A linear map fulfilling the condition in section \ref{projection} is known as a ``projection''.

\subsection{}
Orthogonal projections with respect to some vector subspace are projections of a particular kind, but of course not the only ones we can conceive.

\section{Application 1: Least Squares Regression}


\section{Application 2: Probability}

\subsection{}
The joint probability distribution on the finite sets $X$ and $Y$ sets a probability $p(x,y)$ for each $x\in X$ and $y\in Y$ such that $\sum_{x,y} p(x,y)$ = 1. 

\subsection{}
Define the vector $v\in\R^{|X|\times|Y|}$ with entries $\sqrt{p(x,y)}$. Think of it as reshaping the joint probability table of $p$ into a vector in some convenient order-- this can be formalized using the tensor semantics.

\subsection{}
The orthogonal projection onto $v$ is a rank one linear map that encodes quite a bit of information about the joint probability distribution $p$. Check out the passage from classical probability to quantum probability.


%    Text of article.

%    Bibliographies can be prepared with BibTeX using amsplain,
%    amsalpha, or (for "historical" overviews) natbib style.
\bibliographystyle{amsplain}
%    Insert the bibliography data here.

\end{document}

\comment{

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%    Templates for common elements of a journal article; for additional
%    information, see the AMS-LaTeX instructions manual, instr-l.pdf,
%    included in the PROC author package, and the amsthm user's guide,
%    linked from http://www.ams.org/tex/amslatex.html .

%    Section headings
\section{}
\subsection{}

%    Ordinary theorem and proof
\begin{theorem}[Optional addition to theorem head]
% text of theorem
\end{theorem}

\begin{proof}[Optional replacement proof heading]
% text of proof
\end{proof}

%    Figure insertion; default placement is top; if the figure occupies
%    more than 75% of a page, the [p] option should be specified.
\begin{figure}
\includegraphics{filename}
\caption{text of caption}
\label{}
\end{figure}

%    Mathematical displays; for additional information, see the amsmath
%    user's guide, linked from http://www.ams.org/tex/amslatex.html .

% Numbered equation
\begin{equation}
\end{equation}

% Unnumbered equation
\begin{equation*}
\end{equation*}

% Aligned equations
\begin{align}
  &  \\
  &
\end{align}

%-----------------------------------------------------------------------
% End of proc-l-template.tex
%-----------------------------------------------------------------------
}